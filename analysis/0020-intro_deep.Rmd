---
title: "Conceitos Básicos para Introdução ao Deep Learning"
author: "Rafael Vinicius Curiel"
date: "19/03/2022"
output: html_document
---

## Pacotes e comandos

Carregamento dos pacotes  
```{r message=FALSE, warning=FALSE}
# Este script não utilizará bibliotecas.
```


## Carregamento do objeto

Carregamento da base e padronização dos dados
```{r message=FALSE, warning=FALSE}
data <- read.csv(file = "D:/datasets/winequality-red.csv")
data <- scale(data)
```



### Manipulação

Divisão entre base de teste e base de treino:
```{r}
# Define o índice de observações para o dataset de treino
train_test_split_index <- 0.8 * nrow(data)

# Base para treino e teste
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
```



Distinção das variáveis features da target (qualidade) com transposição
```{r}
#Padronizar dados para melhor performance
train_x <- data.frame(train[1:11])
train_y <- data.frame(train[12])

test_x <- data.frame(test[1:11])
test_y <- data.frame(test[12])

# Transposição da matriz (variáveis nas linhas e observações nas colunas)
train_x <- t(train_x)
train_y <- t(train_y)

test_x <- t(test_x )
test_y <- t(test_y)
```



### Estruturação das funções

1 - Arquitetura da rede:
```{r}
arquitetura <- function(train_x, train_y, neuronios_ocultos) {
  n_x <- dim(train_x)[1]  # Dimensão do conjunto de variáveis de treino (linhas) para entrada
  n_h <- neuronios_ocultos  # Quantidade de neurônios nas camadas escondidas
  n_y <- dim(train_y)[1]  # Dimensão do conjunto de variáveis de treino pra saída
  
  tamanho <- list("n_x" = n_x,
               "n_h" = n_h,
               "n_y" = n_y)
  
  return(tamanho)
}
```


2 - Inicialização dos parâmetros:
```{r}
inicializacao <- function(train_x, tamamnho_rede){
    
    n_x <- tamamnho_rede$n_x  # Qtd de features
    n_h <- tamamnho_rede$n_h  # Qtd de neurônios escondidos (hidden)
    n_y <- tamamnho_rede$n_y  # Qtd de targets
    
    # Matriz de pesos aleatórios (runif) features-hidden    
    W1 <- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01
    
    # Matriz de pesos aleatórios (runif) hidden-target    
    W2 <- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.01
    
    parametros <- list("W1" = W1,
                       "W2" = W2)
    
    return (parametros)
}
```


3 - Função de ativação (sigmoide)
```{r}
sigmoid <- function(x){
    return(1 / (1 + exp(-x)))
}
```


4 - Forward Propagation (responsável pelo fluxo do dado na rede)
```{r}
forwardPropagation <- function(train_x, params, tamamnho_rede){
    
    n_h <- tamamnho_rede$n_h  # Qtd de neurônios escondidos (hidden)
    n_y <- tamamnho_rede$n_y  # Qtd de targets
    
    W1 <- params$W1  # Matriz pesos features-hidden
    W2 <- params$W2  # Matriz pesos hiden-target
    
    
    Z1 <- W1 %*% train_x  
    A1 <- sigmoid(Z1)  # Aplicação da função sigmoide
    Z2 <- W2 %*% A1
    A2 <- sigmoid(Z2)  # Aplicação da função sigmoide
    
    cache <- list("Z1" = Z1,
                  "A1" = A1, 
                  "Z2" = Z2,
                  "A2" = A2)

    return (cache)
}
```


5 - Função custo (MSE - Mean Square Error)
```{r}
computeCost <- function(train_y, cache) {
    
    m <- dim(train_y)[2]  # Qtd observações target
    
    A2 <- cache$A2  # Previsão obtida no forwardPropagation

    custo <- sum((train_y-A2)^2)/m  # Soma do quadrado do erro (y-A2) / Qtd de observações
    
    return (custo)
}
```


6 - Back Propagation (alterações dos pesos com cálculo diferencial)
```{r}
backwardPropagation <- function(train_x, train_y, forwardPropagation, inicializacao, arquitetura){
    
    m <- dim(train_x)[2]  # Qtd observações features
    
    n_x <- arquitetura$n_x  # Qtd de features
    n_h <- arquitetura$n_h  # Qtd de neurônios escondidos (hidden)
    n_y <- arquitetura$n_y  # Qtd de targets

    A2 <- forwardPropagation$A2
    A1 <- forwardPropagation$A1
    W2 <- inicializacao$W2

    
    dZ2 <- A2 - train_y  # Previsão subtraído o valor real
    dW2 <- 1/m * (dZ2 %*% t(A1))  # Quanto deve ser mudada a matriz de pesos hidden-target

    
    dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)  # Ajuste
    dW1 <- 1/m * (dZ1 %*% t(train_x))  # Quanto deve ser mudada a matriz de pesos features-hidden

    
    gradientes <- list("dW1" = dW1, 
                       "dW2" = dW2)
    
    return(gradientes)
}
```


7 - Atualização dos parâmetros
```{r}
atualizacao <- function(gradientes, params, aprendizado){

    W1 <- params$W1  # Matriz de pesos aleatórios features-hidden
    W2 <- params$W2  # Matriz de pesos aleatórios hidden-target
    
    dW1 <- gradientes$dW1  # Quanto deve ser mudada a matriz de pesos features-hidden
    dW2 <- gradientes$dW2  # Quanto deve ser mudada a matriz de pesos hidden-target
    
    
    W1 <- W1 - aprendizado * dW1  # Aplicação da taxa de aprendizado
    W2 <- W2 - aprendizado * dW2  # Aplicação da taxa de aprendizado
    
    atualizado <- list("W1" = W1, "W2" = W2)
    
    return (atualizado)
}
```


8 - Épocas
```{r}
modelo_treino <- function(train_x, train_y, iteracoes, oculto, txaprend){
    
    tamamnho_rede <- arquitetura(train_x, train_y, oculto)
    ini_param <- inicializacao(train_x, tamamnho_rede)

    custo_hist <- c()

    for (i in 1:iteracoes) {
        fwd_prop <- forwardPropagation(train_x, ini_param, tamamnho_rede)
        custo <- computeCost(train_y, fwd_prop)
        back_prop <- backwardPropagation(train_x, train_y, fwd_prop, ini_param, tamamnho_rede)
        atualiza <- atualizacao(back_prop, ini_param, aprendizado = txaprend)
        ini_param <- atualiza
        custo_hist <- c(custo_hist, custo)
        
    }
    
    saida <- list("atualiza" = atualiza,
                      "custo_hist" = custo_hist)

    return (saida)
}
```


### Aplicação do modelo

```{r}
epocas = 100
neuronios_ocultos = 40
taxa_aprendizado = 0.01

modelo_treino <- modelo_treino(train_x,
                               train_y, 
                               oculto = neuronios_ocultos, 
                               iteracoes = epocas, 
                               txaprend = taxa_aprendizado)
```


### Teste dos resultados
```{r}
#Gera previsões
tamamnho_rede <- arquitetura(test_x, test_y, neuronios_ocultos)
params <- modelo_treino$atualiza
fwd_prop <- forwardPropagation(test_x, params, tamamnho_rede)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)


#Verifica função custo
plot(modelo_treino$custo_hist)
```
